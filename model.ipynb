{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yacht import Battle\n",
    "import numpy as np\n",
    "from keras import layers, Input, optimizers, losses\n",
    "from keras.models import Model\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import tensorflow as tf\n",
    "gpu_available = tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x):\n",
    "    return np.where(x>0,1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YachtGAN(Model):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def call(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot2fix(hot, cur_dice):\n",
    "    '''\n",
    "    hot(5,) is np.array((1,1,0,0,1)) --> len 5, tell which dice should fix\n",
    "    cur_dice(6,) is apparently current dice.\n",
    "    '''\n",
    "    fix = np.zeros(6)\n",
    "    j=1#j is value of dice\n",
    "    for i in range(5):#i is count of dice\n",
    "        while sum(cur_dice[:j])<i+1:\n",
    "            j+=1\n",
    "        if hot[i]:\n",
    "            fix[j-1]+=1\n",
    "    return fix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dice (InputLayer)               [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dice_remain (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 7)            0           dice[0][0]                       \n",
      "                                                                 dice_remain[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           512         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "score_selected (InputLayer)     [(None, 12)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 76)           0           dense_2[0][0]                    \n",
      "                                                                 score_selected[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           4928        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dice_fix (Dense)                (None, 5)            385         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "choose (Dense)                  (None, 12)           780         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,605\n",
      "Trainable params: 6,605\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dice_input = Input(shape=(6,), name = 'dice')\n",
    "dice_remain_input = Input(shape=(1,), name = 'dice_remain')\n",
    "score_selected_input = Input(shape=(12,), name = 'score_selected')\n",
    "\n",
    "# score_board_input = Input(shape=(12,), dtype = 'int32', name = 'score_board')\n",
    "x = layers.concatenate([dice_input, dice_remain_input])\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.concatenate([x, score_selected_input])\n",
    "dice_fix_output = layers.Dense(5,activation='one_hot', dynamic = True, name = 'dice_fix')(x)#When you use custom activation, use dynamic\n",
    "x= layers.Dense(64, activation = 'relu')(x)\n",
    "choose_output = layers.Dense(12, activation = 'softmax', name = 'choose')(x)\n",
    "model = Model([dice_input, dice_remain_input, score_selected_input], [dice_fix_output, choose_output])\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=0.01)\n",
    "huber_loss = losses.Huber()\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "rewards_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437.7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def cow_expect(cow_model,n=20):\n",
    "    loss=0\n",
    "    for i in range(n):\n",
    "        cow=battle()\n",
    "        dice_fix, choose = cow_model([\n",
    "            tf.expand_dims(cow.dice,0),\n",
    "            tf.expand_dims(cow.dice_remain,0),\n",
    "            tf.expand_dims(cow.score_selected,0)])\n",
    "        for turn in range(12):\n",
    "            for roll in range(3):\n",
    "                dice_fix, choose = cow_model([\n",
    "                    tf.expand_dims(cow.dice,0),\n",
    "                    tf.expand_dims(cow.dice_remain,0),\n",
    "                    tf.expand_dims(cow.score_selected,0)])\n",
    "                choose = np.array(choose)\n",
    "                while cow.score_selected[choose[0].argmax()]:\n",
    "                    choose[0][choose[0].argmax()]=0\n",
    "                    loss+=1\n",
    "                if sum(dice_fix[0])==5 or roll==2: break\n",
    "                fix = hot2fix(dice_fix[0], cow.dice)\n",
    "                cow.roll(fix=fix.astype(int))\n",
    "            cow.turn(choose[0].argmax())\n",
    "        loss+=cow.total_score()\n",
    "    return loss\n",
    "cow_expect(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(81209.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['dense_2/kernel:0', 'dense_2/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'dice_fix/kernel:0', 'dice_fix/bias:0', 'choose/kernel:0', 'choose/bias:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-7bc3bb3b125c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\EnvKeras\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    511\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mnone\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m     \"\"\"\n\u001b[1;32m--> 513\u001b[1;33m     \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_filter_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    514\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\EnvKeras\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_filter_grads\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m   1268\u001b[0m   \u001b[0mfiltered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1270\u001b[1;33m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0m\u001b[0;32m   1271\u001b[0m                      ([v.name for _, v in grads_and_vars],))\n\u001b[0;32m   1272\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: ['dense_2/kernel:0', 'dense_2/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'dice_fix/kernel:0', 'dice_fix/bias:0', 'choose/kernel:0', 'choose/bias:0']."
     ]
    }
   ],
   "source": [
    "n=20\n",
    "for i in range(1000):  # Run until solved\n",
    "    episode_reward = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss=tf.constant(0.)\n",
    "        for i in range(n):\n",
    "            cow=battle()\n",
    "            dice_fix, choose = model([\n",
    "                tf.expand_dims(cow.dice,0),\n",
    "                tf.expand_dims(cow.dice_remain,0),\n",
    "                tf.expand_dims(cow.score_selected,0)])\n",
    "            for turn in range(12):\n",
    "                for roll in range(3):\n",
    "                    dice_fix, choose = model([\n",
    "                        tf.expand_dims(cow.dice,0),\n",
    "                        tf.expand_dims(cow.dice_remain,0),\n",
    "                        tf.expand_dims(cow.score_selected,0)])\n",
    "                    choose = np.array(choose)\n",
    "                    while cow.score_selected[choose[0].argmax()]:\n",
    "                        choose[0][choose[0].argmax()]=0\n",
    "                        loss+=1\n",
    "                    if sum(dice_fix[0])==5 or roll==2: break\n",
    "                    fix = hot2fix(dice_fix[0], cow.dice)\n",
    "                    cow.roll(fix=fix.astype(int))\n",
    "                loss+=350 - cow.total_score()\n",
    "                cow.turn(choose[0].argmax())\n",
    "    print(loss)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cow_Agent():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.weight_backup      = \"cartpole_weight.h5\"\n",
    "        self.state_size         = state_size\n",
    "        self.action_size        = action_size\n",
    "        self.memory             = deque(maxlen=2000)\n",
    "        self.learning_rate      = 0.001\n",
    "        self.gamma              = 0.95\n",
    "        self.exploration_rate   = 1.0\n",
    "        self.exploration_min    = 0.01\n",
    "        self.exploration_decay  = 0.995\n",
    "        self.brain              = self._build_model()\n",
    "\n",
    "    def cow_expect(cow_model,n=20):\n",
    "\n",
    "    total_loss = 0\n",
    "    for i in range(n):\n",
    "        cow=battle()\n",
    "        loss=0\n",
    "        dice_fix, choose = cow_model([\n",
    "            tf.expand_dims(cow.dice,0),\n",
    "            tf.expand_dims([cow.dice_remain],0),\n",
    "            tf.expand_dims(cow.score_selected,0)])\n",
    "        for turn in range(12):\n",
    "            for roll in range(3):\n",
    "                dice_fix, choose = cow_model([\n",
    "                    tf.expand_dims(cow.dice,0),\n",
    "                    tf.expand_dims(cow.dice_remain,0),\n",
    "                    tf.expand_dims(cow.score_selected,0)])\n",
    "                choose = np.array(choose)\n",
    "                while cow.score_selected[choose[0].argmax()]:\n",
    "                    choose[0][choose[0].argmax()]=0\n",
    "                    loss+=1\n",
    "                if sum(dice_fix[0])==5 or roll==2: break\n",
    "                fix = hot2fix(dice_fix[0], cow.dice)\n",
    "                cow.roll(fix=fix.astype(int))\n",
    "            cow.turn(choose[0].argmax())\n",
    "        total_loss+=cow.total_score()\n",
    "    loss += 350 - total_loss/n\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while True:  # Run until solved\n",
    "    episode_reward = 0\n",
    "    cow = battle()\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss=0\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            loss_value = cow_expect(model)\n",
    "                \n",
    "\n",
    "            action_probs, critic_value = model(state)\n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "            # Sample action from action probability distribution\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Update running reward to check condition for solving\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # Calculate expected value from rewards\n",
    "        # - At each timestep what was the total reward received after that timestep\n",
    "        # - Rewards in the past are discounted by multiplying them with gamma\n",
    "        # - These are the labels for our critic\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        # Normalize\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        # Calculating loss values to update our network\n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            # At this point in history, the critic estimated that we would get a\n",
    "            # total reward = `value` in the future. We took an action with log probability\n",
    "            # of `log_prob` and ended up recieving a total reward = `ret`.\n",
    "            # The actor must be updated so that it predicts an action that leads to\n",
    "            # high rewards (compared to critic's estimate) with high probability.\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "            # The critic must be updated so that it predicts a better estimate of\n",
    "            # the future rewards.\n",
    "            critic_losses.append(\n",
    "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "            )\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Clear the loss and reward history\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "\n",
    "    # Log details\n",
    "    episode_count += 1\n",
    "    if episode_count % 10 == 0:\n",
    "        template = \"running reward: {:.2f} at episode {}\"\n",
    "        print(template.format(running_reward, episode_count))\n",
    "\n",
    "    if running_reward > 195:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb780940523a8c094ea2ed6945bfdd0a9881604c734fac175651737e334280f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
